{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent with LangSmith Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<center><img src=\"./images/time_agent.png\" width=\"600\"/></center>\n",
    "</div>\n",
    "\n",
    "This exercise builds upon the first AI agent we created in **Notebook 1 - My First AI Agent**. We'll recreate the same ReAct AI agent using [**LangGraph**](https://www.langchain.com/langgraph), but this time we'll add **LangSmith tracing** to monitor and debug the agent's behavior in real-time.\n",
    "\n",
    "The agent will have the same task: **providing the current date and time in a specific location**. It will respond to questions such as:\n",
    "\n",
    "**\"What time is it in Italy?\"**\n",
    "\n",
    "### What's New in This Exercise:\n",
    "\n",
    "The key difference is the integration of **LangSmith tracing**, which allows you to:\n",
    "- **Visualize the agent's execution flow**: See each step the agent takes, from receiving input to generating a response.\n",
    "- **Debug tool calls**: Monitor which tools are invoked, with what parameters, and what they return.\n",
    "- **Track performance metrics**: Measure latency, token usage, and other important metrics.\n",
    "- **Analyze conversation history**: Review past interactions to understand agent behavior patterns.\n",
    "\n",
    "LangSmith provides a powerful observability platform for LLM applications, making it easier to understand, debug, and optimize your AI agents.\n",
    "\n",
    "Let's get started with setting up tracing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparing the Environment and Setting Up LangSmith Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the AI agent, we need to set up our environment and import the necessary functions and libraries. In this exercise, we will use:\n",
    "\n",
    "- [`load_dotenv`](https://pypi.org/project/python-dotenv/): To load environment variables (e.g., your OpenAI API key).\n",
    "- [`ChatOpenAI`](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html): A language model interface for interacting with OpenAI's GPT models.\n",
    "- [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/): A utility from **LangGraph** for building ReAct AI agents.\n",
    "- [`tool`](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html): A decorator to define tools that the agent can use during execution.\n",
    "- `print_stream`: A utility to print the agent’s responses as they stream in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up LangSmith Tracing\n",
    "\n",
    "To enable LangSmith tracing for your AI agent, you need to configure the following environment variables in your `.env` file:\n",
    "\n",
    "**Required Environment Variables:**\n",
    "- **`LANGSMITH_TRACING`**: Set to `\"true\"` to enable tracing.\n",
    "- **`LANGSMITH_API_KEY`**: Your LangSmith API key (obtain it from [smith.langchain.com](https://smith.langchain.com)).\n",
    "- **`OPENAI_API_KEY`**: Your OpenAI API key (or your LLM provider's API key).\n",
    "\n",
    "**Optional Environment Variables:**\n",
    "- **`LANGSMITH_WORKSPACE_ID`**: If your LangSmith account is linked to multiple workspaces, specify which workspace to use.\n",
    "- **`LANGSMITH_PROJECT`**: The project name under which traces will be organized (defaults to \"default\" if not specified).\n",
    "\n",
    "### Example `.env` Configuration:\n",
    "```\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_API_KEY=your_langsmith_api_key_here\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "LANGSMITH_PROJECT=tech4tech-ai-agents\n",
    "```\n",
    "\n",
    "Once these variables are set, LangSmith will automatically capture and send tracing data every time your agent runs. You can view the traces in the LangSmith dashboard at [smith.langchain.com](https://smith.langchain.com).\n",
    "\n",
    "**Note**: Make sure you have a LangSmith account. You can sign up for free at [smith.langchain.com](https://smith.langchain.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from utils.stream import print_stream\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing the Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to create a new AI Agent with LangGraph is to initialize the OpenAI language model that our agent will use to process and respond to queries. When specifying the model, it's a good practice to:\n",
    "\n",
    "1. **Specify the full model name**: To ensure compatibility and stability (e.g., `gpt-4o-mini-2024-07-18`).\n",
    "2. **Start with a low temperature value**: A temperature of `0` ensures deterministic responses, making the agent focus on precision. You can increase the temperature later for more creative responses.\n",
    "\n",
    "For this exercise, we will use the latest version of the `gpt-4o-mini` model (`gpt-4o-mini-2024-07-18`) with a temperature of `0`, but you can experiment with other models (the full list is available [here](https://platform.openai.com/docs/models#current-model-aliases)), temperatures, and other parameters to see how they affect the agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai:gpt-4o-mini-2024-07-18\"\n",
    "MODEL_TEMPERATURE = 0\n",
    "\n",
    "llm = init_chat_model(model=MODEL_NAME, temperature=MODEL_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about ChatOpenAI in the official langchain documentation: https://python.langchain.com/docs/integrations/chat/openai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before equipping our agent with tools, let’s verify that the underlying language model lacks access to real-time information such as the current date and time. This is expected behavior because the LLM operates on static knowledge and does not have tools or APIs to fetch live data on its own.\n",
    "\n",
    "We’ll confirm this by asking the LLM the following question:  \n",
    "\n",
    "**\"What time is it in Italy?\"**\n",
    "\n",
    "Let’s run the code and observe the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can learn more about langchain messages and roles in the official documentation: https://python.langchain.com/docs/concepts/messages/\n",
    "messages = [\n",
    "    (\"user\", \"What time is it in Italy?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"Assistant:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen that the LLM cannot access real-time information on its own, we can give the AI agent a **tool** to fill this gap. Tools are external functions that the agent can use to assist in answering user queries.\n",
    "\n",
    "### Tool Requirements:\n",
    "\n",
    "For this exercise, we are building a tool to retrieve the current date and time for a given timezone. Here's what the tool should do:\n",
    "1. Accept an input, specifically a timezone.\n",
    "2. Determine the current date and time for that timezone.\n",
    "3. Return the result to the agent.\n",
    "4. Manage any errors that may occur during the process.\n",
    "\n",
    "You can define your own implementation of the tool (recomended) or use the provided one.\n",
    "\n",
    "To learn more about tools in LangGraph, you can refer to the official documentation: https://python.langchain.com/docs/concepts/tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.date_and_time import prebuilt_get_current_datetime\n",
    "\n",
    "@tool\n",
    "def get_current_datetime(timezone: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current date and time in a specified timezone.\n",
    "\n",
    "    Args:\n",
    "        timezone (str): The timezone to get the current date and time for (e.g., \"Europe/Rome\").\n",
    "    Returns:\n",
    "        str: The current date and time in the specified timezone.\n",
    "    \"\"\"\n",
    "    return prebuilt_get_current_datetime(timezone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the tool by calling it with a sample timezone, such as `\"Europe/Rome\"`, and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_current_datetime.invoke(\"Europe/Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crafting the Agent System Prompt\n",
    "\n",
    "Before integrating tools and building our agent, we need to define the **system prompt**. The system prompt acts as the foundation for the agent's behavior by clearly outlining its role, purpose, instructions, and limitations.\n",
    "\n",
    "Crafting the perfect system prompt is an iterative process, so you can start with a simple version and refine it as you test the agent's performance.\n",
    "\n",
    "### Prompt Engineering Techniques:\n",
    "Here are some techniques to keep in mind when defining the agent's system prompt:\n",
    "1. **Role Prompting**: Clearly define the role of the agent (e.g., \"You are D&T Bot, an agent specialized in providing the current date and time for different locations.\").\n",
    "2. [**Few-Shot Prompting**](https://www.promptingguide.ai/techniques/fewshot) (Optional): Provide a few examples of user questions and the appropriate responses to guide the agent further.\n",
    "\n",
    "You can learn more about prompt engineering in the prompting engineering guide: [https://www.promptingguide.ai/](https://www.promptingguide.ai/)\n",
    "\n",
    "### Your Task:\n",
    "Define a system prompt for your agent. The agent should:\n",
    "- Clearly convey its **role and purpose**.\n",
    "- Provide the **current date and time** for a specific location when asked.\n",
    "- Ask the user to specify a location if it is not provided.\n",
    "- Politely decline to answer any queries unrelated to date and time.\n",
    "\n",
    "### Things to Consider:\n",
    "- **No Need to Describe Tools**: You do not need to specify the tools or their behaviors in the system prompt. The agent will already know how to use them based on the descriptions you provided earlier.\n",
    "- **Be Creative**: You can inject some personality into your prompt if desired to give your agent a unique tone (e.g., friendly, formal, or professional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_AND_TIME_AGENT_PROMPT = \"\"\"\n",
    "You are a helpful AI assistant that can provide the current date and time in various timezones. You have access to the following tool:\n",
    "get_current_datetime: Get the current date and time in a specified timezone.\n",
    "\n",
    "When you receive a question, determine if you need to use the tool to answer it. If you do, use the tool and provide the answer. If you don't need to use the tool, answer the question directly.\n",
    "\n",
    "Tone: Friendly tone, use emojis, and be concise.\n",
    "Response Format: Always respond in a single line, either with the tool's output or a direct answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the AI Agent\n",
    "\n",
    "Now that we have all the core components ready:\n",
    "1. The **language model (LLM)** for processing queries.\n",
    "2. A **tool** for retrieving the current date and time.\n",
    "3. A well-crafted **system prompt** to guide the agent's behavior.\n",
    "\n",
    "We’re ready to bring everything together into a fully functional AI agent. \n",
    "\n",
    "### How It Works:\n",
    "We’ll use the **`create_react_agent`** function from LangGraph. This utility embeds the logic required to create a basic **ReAct (Reason + Act)** agent, allowing it to:\n",
    "- Reason about the user’s query.\n",
    "- Decide whether and how to use the provided tools.\n",
    "- Generate meaningful responses, respecting the constraints of the system prompt.\n",
    "\n",
    "### Key Parameters:\n",
    "- **`model`**: The LLM we defined earlier.\n",
    "- **`tools`**: A list of tools (e.g., the `get_current_datetime` tool).\n",
    "- **`prompt`**: The system prompt crafted to guide the agent.\n",
    "\n",
    "You can learn more about the `create_react_agent` function in the official LangGraph documentation: https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=[get_current_datetime], \n",
    "    prompt=DATE_AND_TIME_AGENT_PROMPT, \n",
    "    checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ReAct agent** we just created isn't just a simple function; it's a structured graph that represents how the agent reasons, acts, and responds to user input. LangGraph allows us to visualize this graph, which helps us better understand:\n",
    "- How the agent processes inputs.\n",
    "- How its decision-making workflow is structured.\n",
    "\n",
    "Let’s display the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the AI Agent\n",
    "\n",
    "Now that the agent is fully built, it's time to test it! By sending a query to the agent, we can examine how it:\n",
    "1. Processes the input question.\n",
    "2. Reasonably decides if and how to use tools like `get_current_datetime`.\n",
    "3. Produces an accurate and user-friendly response.\n",
    "\n",
    "### Testing Query:\n",
    "Let’s try the same query we used to test the LLM:\n",
    "\n",
    "**\"What time is it in Italy?\"**\n",
    "\n",
    "To verify that the agent is now capable of providing the current date and time for a specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "question = \"\"\"What time is it in Italy?\"\"\"\n",
    "inputs = {\"messages\": [(\"user\", question)]}\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": str(uuid4())\n",
    "        }\n",
    "}\n",
    "\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more complex query:\n",
    "\n",
    "**\"What time is it in Italy and California?\"**\n",
    "\n",
    "This query is intentionally a little tricky so we can observe how the agent handles both locations and whether it uses tools appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"What time is it in Italy and California?\"\"\"\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", question)]}\n",
    "\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Testing:\n",
    "\n",
    "Now that your agent is up and running with LangSmith tracing enabled, it's time to test it with your own queries! This hands-on testing allows you to evaluate:\n",
    "1. **How well the agent responds to expected queries** (e.g., asking the time in different locations).\n",
    "2. **Whether it handles unexpected or ambiguous queries gracefully** (e.g., if no location is provided or irrelevant questions are asked).\n",
    "3. **How effectively it uses the tools** to fulfill its purpose.\n",
    "\n",
    "### Monitoring with LangSmith:\n",
    "\n",
    "While testing, **open your LangSmith dashboard** at [smith.langchain.com](https://smith.langchain.com) to observe:\n",
    "- **Real-time traces**: Watch each agent execution as it happens.\n",
    "- **Tool invocations**: See which tools are called, with what parameters, and what they return.\n",
    "- **Token usage and costs**: Monitor how many tokens each interaction consumes.\n",
    "- **Latency metrics**: Identify performance bottlenecks in your agent's workflow.\n",
    "- **Error tracking**: Quickly spot and debug any failures or unexpected behaviors.\n",
    "\n",
    "### Iterative Process:\n",
    "\n",
    "Crafting the perfect agent is an **iterative process**. Use these trials to identify areas of improvement and adjust the agent as needed. Consider modifying the following aspects:\n",
    "- **System Prompt**: Adjust the wording to clarify the agent's purpose, add reasoning steps, or inject personality.\n",
    "- **Tool Definition**: Rename tools, change their parameters, or improve their descriptions for clarity and better integration.\n",
    "- **Model Settings**: Experiment with model temperature to balance determinism (`temperature=0`) and creativity (higher values).\n",
    "- **Tool Limitations**: For example, add error handling if an invalid timezone is passed.\n",
    "\n",
    "### Tips for Effective Refinement:\n",
    "- Use **LangSmith traces** to review the agent's reasoning steps and identify where improvements are needed.\n",
    "- Compare multiple runs in LangSmith to understand how changes affect behavior.\n",
    "- Test with a wider variety of queries to account for edge cases.\n",
    "- Iterate gradually, making small adjustments to prompts, tools, or configurations one step at a time.\n",
    "\n",
    "Remember, building an AI agent is like sculpting—it evolves over time with refinement and experimentation. LangSmith makes this process more transparent and efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"<YOUR QUESTION HERE>\"\"\"\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", question)]}\n",
    "\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "🎉 Congratulations on Completing the Exercise! 🎉\n",
    "\n",
    "Well done! You've successfully built your first AI agent with **LangSmith tracing** enabled. This exercise demonstrated how to:\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. **Set up LangSmith tracing**: You configured the necessary environment variables to enable real-time monitoring of your AI agent.\n",
    "2. **Built a traced AI agent**: You recreated the date and time agent from Notebook 1, but now with full observability through LangSmith.\n",
    "3. **Monitored agent execution**: You learned how to use the LangSmith dashboard to observe traces, tool calls, token usage, and performance metrics.\n",
    "4. **Debugged with visibility**: You saw how tracing makes it easier to understand what your agent is doing at each step and identify areas for improvement.\n",
    "\n",
    "### The Power of LangSmith Tracing:\n",
    "\n",
    "LangSmith transforms the development experience by providing:\n",
    "- **Complete visibility** into every step of your agent's execution\n",
    "- **Performance insights** to optimize speed and cost\n",
    "- **Debugging capabilities** to quickly identify and fix issues\n",
    "- **Collaboration tools** to share traces with team members\n",
    "- **Production monitoring** for deployed applications\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You're now ready to apply LangSmith tracing to more complex agents! In the **next notebook (6 - Multi-agent with tracing.ipynb)**, you'll explore how to:\n",
    "- Trace multi-agent systems with multiple interacting agents\n",
    "- Monitor complex workflows with multiple tools execution\n",
    "- Debug coordination between different agents\n",
    "\n",
    "LangSmith becomes even more valuable as your agents grow in complexity, making it an essential tool for production AI applications.\n",
    "\n",
    "🚀 Continue to the next notebook to see LangSmith in action with multi-agent systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
