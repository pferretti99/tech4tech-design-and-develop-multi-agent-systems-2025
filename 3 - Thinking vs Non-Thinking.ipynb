{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ae1391",
   "metadata": {},
   "source": [
    "# Notebook 3 (Lecture 2) - Thinking vs Non-Thinking Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd307c1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this section we define and compare two agent variants that share the same ReAct tool set but differ in how the underlying LLM allocates cognitive effort.\n",
    "\n",
    "- Non-Thinking (Standard) Model: produces a direct answer in (roughly) a single forward pass; minimal explicit reasoning; favors speed and cost.\n",
    "- Thinking (Reasoning) Model: allocates extra internal computation (deliberation steps / structured scratchpad) before emitting the final answer; better for multi-step logic, composition, or fragile numeric reasoning.\n",
    "\n",
    "### Objectives\n",
    "1. Understand operational differences between a standard (single-pass) model and a reasoning-enabled model.  \n",
    "2. Observe how tool usage (ReAct loop) changes: fewer calls vs planned multi-step invocations.  \n",
    "3. Learn decision criteria: when the extra latency/cost of reasoning yields higher reliability or clarity.  \n",
    "4. Build intuition for error modes (hallucination vs arithmetic slip vs tool misuse).  \n",
    "\n",
    "### Why This Matters\n",
    "As you scale multi-agent systems, choosing when to pay for deeper reasoning is an optimization problem across latency, budget, and accuracy. A common production pattern is a cascading strategy: default to the fast model; escalate to a reasoning model only when signals (task complexity, multi-step arithmetic, ambiguity, failure retries) exceed a threshold.\n",
    "\n",
    "### Conceptual Contrast\n",
    "**Non-Thinking (Direct)**  \n",
    "- Single-shot generation; little or no iterative self-reflection.  \n",
    "- Lower latency, cheaper, predictable with temperature=0.  \n",
    "- Ideal for: factual lookup, formatting, simple arithmetic, routing, high-volume batch tasks.  \n",
    "- Failure pattern: confidently skips required decomposition.\n",
    "\n",
    "**Thinking (Reasoning-Enhanced)**  \n",
    "- Allocates internal tokens to plan, decompose, and verify.  \n",
    "- Better at multi-step numeric or conditional logic, combining tool outputs, resolving ambiguity.  \n",
    "- Ideal for: chained calculations, comparisons, lightweight analytical explanation, structured transformation.  \n",
    "- Failure pattern: unnecessary over-elaboration or marginal extra cost on trivial prompts.\n",
    "\n",
    "### Evaluation Dimensions\n",
    "| Dimension | Non-Thinking | Thinking |\n",
    "|----------|--------------|----------|\n",
    "| Latency | Faster | Slower |\n",
    "| Cost (tokens/compute) | Lower | Higher |\n",
    "| Reliability (simple tasks) | High | Similar |\n",
    "| Multi-step correctness | Moderate | Higher |\n",
    "| Interpretability | Low (no rationale) | Moderate (concise rationale) |\n",
    "| Tool orchestration | Minimal | More deliberate sequencing |\n",
    "\n",
    "### What You'll Build\n",
    "You will instantiate two ReAct agents sharing the same tools (e.g., arithmetic + sequence generator) but backed by different model configurations:\n",
    "- Non-Thinking Agent: concise, direct answers.\n",
    "- Thinking Agent: brief structured reasoning + final answer.\n",
    "\n",
    "You will run identical tasks through both and compare:\n",
    "- Number and order of tool calls\n",
    "- Latency and token usage\n",
    "- Output style and correctness on composite tasks\n",
    "\n",
    "### Ready to Get Started?\n",
    "Next we prepare the environment and register the tools that both agents will leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fb14b",
   "metadata": {},
   "source": [
    "### 0. Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f182f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import e setup ambiente\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from utils.stream import display_stream\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b1fa1",
   "metadata": {},
   "source": [
    "### 1. Tool Definition\n",
    "\n",
    "We now define a minimal, deterministic tool set that both agents (non-thinking vs thinking) will share inside the ReAct loop. The goal is to observe *behavioral differences* (planning, tool sequencing, intermediate reasoning) without the noise of complex business logic.\n",
    "\n",
    "#### Tools Provided\n",
    "1. add: baseline integer addition.\n",
    "2. multiply: integer product for composite arithmetic.\n",
    "3. fibonacci: generates the first n Fibonacci numbers.\n",
    "4. sum_fibonacci: computes the sum of first n Fibonacci numbers. \n",
    "\n",
    "#### Why Fibonacci?\n",
    "- Produces a known numeric sequence → good for testing whether the reasoning model chooses to call the tool rather than recall values.\n",
    "- Encourages multi-step decomposition (generate → take subset → sum/compare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a (int): First integer.\n",
    "        b (int): Second integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of a and b.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a (int): First factor.\n",
    "        b (int): Second factor.\n",
    "\n",
    "    Returns:\n",
    "        int: The product a * b.\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def fibonacci(n: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Generate the first n Fibonacci numbers.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of terms to generate (n >= 1).\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List containing the first n Fibonacci numbers starting at 0.\n",
    "    \"\"\"\n",
    "    out = [0, 1]\n",
    "    while len(out) < n:\n",
    "        out.append(out[-1] + out[-2])\n",
    "    return out[:n]\n",
    "\n",
    "def sum_fibonacci(n: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the sum of the first n Fibonacci numbers.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of terms to sum (n >= 1).\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of the first n Fibonacci numbers.\n",
    "    \"\"\"\n",
    "    fib_sequence = fibonacci({'n':n})\n",
    "    return sum(fib_sequence)\n",
    "\n",
    "# TOOLS = [add, multiply, fibonacci, sum_fibonacci]\n",
    "TOOLS = [add, multiply, fibonacci]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba627c1",
   "metadata": {},
   "source": [
    "### 2. Non-Thinking Agent\n",
    "\n",
    "This agent uses a fast LLM with no internal “thinking budget”. It aims for minimal latency and cost, producing a direct answer unless a tool call is clearly required.\n",
    "\n",
    "Workflow:\n",
    "1. Initialize the lightweight model (temperature=0 for determinism).\n",
    "2. Provide a concise instruction prompt focused on clear, short answers.\n",
    "3. Create the ReAct agent by binding the shared tool set (add, multiply, fibonacci).\n",
    "4. Run test queries and observe\n",
    "\n",
    "When to use:\n",
    "- Simple arithmetic\n",
    "- Direct factual formatting\n",
    "- High-throughput / low-latency pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed29fee",
   "metadata": {},
   "source": [
    "#### 1. Initializing the Non-Thinking LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b066bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL_MODEL_NAME = \"openai:gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "normal_llm = init_chat_model(NORMAL_MODEL_NAME, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa24d1b",
   "metadata": {},
   "source": [
    "#### 2. Crafting the Agent System Prompt\n",
    "\n",
    "Define a system prompt for your agent. The prompt should instruct the non-thinking agent to respond in a direct, concise, and practical manner, without unnecessary reasoning steps or explanations. This helps ensure fast, predictable outputs focused on clarity and brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e308ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL_AGENT_PROMPT = \"\"\"Sei un assistente per la risoluzione di problemi matematici: rispondi in modo sintetico e chiaro.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b395cce",
   "metadata": {},
   "source": [
    "#### 3. Building the AI Agent\n",
    "\n",
    "Now that you have the tools and system prompt ready, it’s time to build the AI agent. The agent should have access to both tools and should also have short-term memory to keep track of the conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89695b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "normal_agent = create_react_agent(\n",
    "    model=normal_llm,\n",
    "    tools=TOOLS,\n",
    "    prompt=NORMAL_AGENT_PROMPT,\n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd28193",
   "metadata": {},
   "source": [
    "#### 4. Testing the Non-Thinking Agent\n",
    "\n",
    "Now that the agent is fully built, it's time to test it! Interact with the agent using a variety of prompts to see how well it can assist users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422004fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_config = {\"configurable\": {\"thread_id\": str(uuid4())}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_task = \"Calcola la somma dei primi 20 numeri di Fibonacci.\"\n",
    "inputs = {\"messages\": [(\"user\", user_task)]}\n",
    "\n",
    "\n",
    "display_stream(normal_agent.stream(inputs, normal_config, stream_mode=\"values\"), thinking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9d06a",
   "metadata": {},
   "source": [
    "### 3. Thinking Agent\n",
    "\n",
    "This agent uses a reasoning-capable LLM with an internal “thinking budget” (extra deliberation steps before finalizing the answer). It trades higher latency and cost for improved reliability on multi-step or ambiguous tasks, and tends to orchestrate tools more deliberately (plan → call → verify → answer) while still producing a concise final response.\n",
    "\n",
    "Workflow:\n",
    "1. Initialize the reasoning model (set a reasoning/effort parameter, e.g. medium).\n",
    "2. Provide a system prompt encouraging brief structured reasoning, explicit tool use when needed, and concise final answers.\n",
    "3. Create the ReAct agent with the same shared tools (add, multiply, fibonacci).\n",
    "4. Run comparative queries and observe: more frequent multi-step tool calls, clearer decomposition, reduced arithmetic/slip risk.\n",
    "\n",
    "When to use:\n",
    "- Multi-step arithmetic or chained calculations\n",
    "- Sequence generation plus aggregation (e.g. sum / compare Fibonacci values)\n",
    "- Conditional or comparative logic (“which is larger and by how much?”)\n",
    "- Ambiguous or partially specified instructions needing clarification\n",
    "- Higher accuracy or auditability requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a73881",
   "metadata": {},
   "source": [
    "#### 1. Initializing the Thinking LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "THINKING_MODEL_NAME = \"openai:o4-mini-2025-04-16\"\n",
    "\n",
    "thinking_llm = init_chat_model(\n",
    "    THINKING_MODEL_NAME,\n",
    "    reasoning={\"effort\": \"medium\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40dc72",
   "metadata": {},
   "source": [
    "#### 2. Crafting the Agent System Prompt\n",
    "\n",
    "Define a system prompt for your reasoning (thinking) agent. It should explicitly instruct the model to:\n",
    "\n",
    "- Perform a brief internal plan (not all steps must be exposed).\n",
    "- Use tools whenever a calculation, sequence generation, comparison, or multi-step numeric operation is required (avoid guessing numbers).\n",
    "- Keep the visible reasoning summary concise (1–3 short lines) and then give the final answer.\n",
    "- Prefer correct, tool-verified results over speed.\n",
    "- Return output in a clean structure: (a) short rationale (optional), (b) final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "THINKING_MODEL_PROMPT = \"\"\"Sei un assistente per la risoluzione di problemi matematici: rispondi in modo sintetico e chiaro.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49100a04",
   "metadata": {},
   "source": [
    "#### 3. Building the AI Agent\n",
    "\n",
    "Now that you have the tools and system prompt ready, it’s time to build the AI agent. The agent should have access to both tools and should also have short-term memory to keep track of the conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f098dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "thinking_agent = create_react_agent(\n",
    "    model=thinking_llm,\n",
    "    tools=TOOLS,\n",
    "    prompt=THINKING_MODEL_PROMPT, \n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad54aed",
   "metadata": {},
   "source": [
    "#### 4. Testing the Thinking Agent\n",
    "\n",
    "Now that the agent is fully built, it's time to test it! Interact with the agent using a variety of prompts to see how well it can assist users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinking_config = {\"configurable\": {\"thread_id\": str(uuid4())}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191db43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_task = \"Calcola la somma dei primi 20 numeri di Fibonacci.\"\n",
    "inputs = {\"messages\": [(\"user\", user_task)]}\n",
    "\n",
    "display_stream(\n",
    "    thinking_agent.stream(inputs, thinking_config, stream_mode=\"values\"),\n",
    "    thinking=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
